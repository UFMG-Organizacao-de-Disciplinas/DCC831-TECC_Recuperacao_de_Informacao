{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1788c6c2",
   "metadata": {},
   "source": [
    "# Programming Assignment #2 - Indexer and Query Processor\n",
    "\n",
    "- **Deadline:** June 2nd, 2025 23:59 via Moodle\n",
    "\n",
    "## Overview\n",
    "\n",
    "The goal of this assignment is to implement the indexer and query processor modules of a web search engine. In addition to the source code of your implementation, your submission must include a characterization of the index built for a mid-sized corpus and the results retrieved for a set of queries.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "You must use Python 3.13 for this assignment. Your code must run in a virtual environment using only the libraries included in the provided `requirements.txt` file. Execution errors due to missing libraries or incompatible library versions will result in a zero grade. To make sure you have the correct setup, you can test it using the following commands\n",
    "\n",
    "```bash\n",
    "python3 -m venv pa2\n",
    "source pa2/bin/activate\n",
    "pip3 install -r /path/to/requirements.txt\n",
    "```\n",
    "\n",
    "## Indexer\n",
    "\n",
    "Your implementation must include an `indexer.py` file, which will be executed in the same virtual environment described above, as follows:\n",
    "\n",
    "```bash\n",
    "python3 indexer.py -m <MEMORY> -c <CORPUS> -i <INDEX>\n",
    "```\n",
    "\n",
    "with the following arguments:\n",
    "\n",
    "- `-m <MEMORY>`: the memory available to the indexer in megabytes.\n",
    "- `-c <CORPUS>`: the path to the corpus file to be indexed.\n",
    "- `-i <INDEX>`: the path to the directory where indexes should be written.\n",
    "\n",
    "At the end of the execution, your indexer.py implementation must print a JSON document to standard output [$^1$][Note_1] with the following statistics:\n",
    "\n",
    "- `Index Size`: the index size in megabytes;\n",
    "- `Elapsed Time`: the time elapsed (in seconds) to produce the index;\n",
    "- `Number of Lists`: the number of inverted lists in the index;\n",
    "- `Average List Size`: the average number of postings per inverted list.\n",
    "\n",
    "The following example illustrates the required output format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Index Size\": 2354,\n",
    "  \"Elapsed Time\": 45235,\n",
    "  \"Number of Lists\": 437,\n",
    "  \"Average List Size\": 23.4\n",
    "}\n",
    "```\n",
    "\n",
    "### Document Corpus\n",
    "\n",
    "The corpus to be indexed comprises structured representations (with id, title, descriptive text, and keywords) for a total of 4,641,784 named entities present in Wikipedia. These structured representations are encoded as `JSON` documents in a single `JSONL` file available for download.[$^2$][Note_2] To speed up development, you are encouraged to use a smaller portion of the corpus to test your implementation before you try to index the complete version.\n",
    "\n",
    "### Indexing Policies\n",
    "\n",
    "For each document in the corpus (the `-c` argument above), your implementation must parse, tokenize, and index it. Your implementation must operate within the designated memory budget (the `-m` argument) during its entire execution. [$^3$][Note_3] [$^4$][Note_4] This emulates the most typical scenario where the target corpus far exceeds the amount of physical memory available to the indexer. At the end of the execution, a final representation of all produced index structures (inverted index, document index, term lexicon) must be stored as three separate files, one for each structure, at the designated directory (the `-i` argument).\n",
    "\n",
    "In addition to this workflow, your implementation **must abide by the following policies**, which will determine your final grade in this assignment:\n",
    "\n",
    "1. _Pre-processing Policy_: To reduce the index size, your implementation **must perform stopword removal and stemming**. Additional preprocessing techniques can be implemented at your discretion.\n",
    "\n",
    "2. _Memory Management Policy_: To ensure robustness, your implementation **must execute under limited memory availability**. To this end, it must be able to produce partial indexes in memory (respecting the imposed memory budget) and merge them on disk. [$^5$][Note_5]\n",
    "\n",
    "3. _Parallelization Policy_: To ensure maximum efficiency, you **must parallelize the indexing process across multiple threads**. You may experiment to find an optimal number of threads to minimize indexing time rate while minimizing the incurred parallelization overhead.\n",
    "\n",
    "4. _Compression Policy (extra)_: Optionally, **you may choose to implement a compression scheme for index entries** (e.g. gamma for docids, unary for term frequency) for maximum storage efficiency.\n",
    "\n",
    "[Note_1]: https://en.wikipedia.org/wiki/Standard_streams#Standard_output_(stdout)\n",
    "[Note_2]: https://www.kaggle.com/datasets/rodrygo/entities\n",
    "\n",
    "[Note_3]: <> \"The memory limit will be strictly enforced during grading. If your program exceeds it, it may be automatically terminated with an out-of-memory (OOM) error. To prevent this, use `psutil.Process(os.getpid()).memory_info().rss` to monitor your current memory usage (in bytes), and offload partial indexes to disk before allocating more memory as needed.\"\n",
    "\n",
    "[Note_4]: <> \"Note that the memory budget refers to the total memory available to your implementation, not only to the memory needed to store the actual index structures. As a reference lower bound, assume your implementation will be tested with `-m 1024`.\"\n",
    "\n",
    "[Note_5]: https://en.wikipedia.org/wiki/External_sorting#External_merge_sort\n",
    "\n",
    "## Query Processor\n",
    "\n",
    "Your implementation must include a `processor.py` file, which will be executed in the previously described environment, as follows:\n",
    "\n",
    "```bash\n",
    "python3 processor.py -i <INDEX> -q <QUERIES> -r <RANKER>\n",
    "```\n",
    "\n",
    "with the following arguments:\n",
    "\n",
    "- `-i <INDEX>`: the path to an index file.\n",
    "- `-q <QUERIES>`: the path to a file with the list of queries to process.\n",
    "- `-r <RANKER>`: a string informing the ranking function (either “TFIDF” or “BM25”) to be used to score documents for each query.\n",
    "\n",
    "After processing **each query** (the `-q` argument above), your `processor.py`\n",
    "implementation must print a `JSON` document to standard output [$^6$][Note_6] with the top\n",
    "results retrieved for that query according to the following format:\n",
    "\n",
    "- `Query`, the query text;\n",
    "- `Results`, a list of results.\n",
    "\n",
    "Each result in the Results list must be represented with the fields:\n",
    "\n",
    "- `ID`, the respective result ID;\n",
    "- `Score`, the final document score.\n",
    "\n",
    "The following example illustrates the required output format for a query:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Query\": \"information retrieval\",\n",
    "  \"Results\": [\n",
    "    { \"ID\": \"0512698\", \"Score\": 24.2},\n",
    "    { \"ID\": \"0249777\", \"Score\": 12.4},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "[Note_6]: https://en.wikipedia.org/wiki/Standard_streams#Standard_output_(stdout)\n",
    "\n",
    "The results list for a query must be sorted in reverse document score order and include up to the top 10 results retrieved for that query.\n",
    "\n",
    "### Query Processing Policies\n",
    "\n",
    "For each query in the list provided via the `-q` argument, your implementation must pre-process the query, retrieve candidate documents from the given index (the `-i` argument), score these documents according to the chosen ranking model (the `-r` argument), and print the top 10 results using the aforementioned format. In addition to this standard workflow, **your implementation must abide by the following policies**:\n",
    "\n",
    "1. _Pre-processing Policy_: Your implementation **must pre-process queries with stopword removal and stemming**. This policy should be aligned with the implemented document pre-processing policy for indexing to correctly match queries with documents.\n",
    "2. _Matching Policy_: For improved efficiency, your implementation **must perform a conjunctive document-at-a-time (DAAT) matching** when retrieving candidate documents.\n",
    "3. _Scoring Policy_: Your implementation **must provide two scoring functions: TFIDF and BM25**. You are free to experiment with different variants of these functions from the literature.\n",
    "4. _Parallelization Policy (extra)_: To ensure maximum efficiency, you **may parallelize the query processing across multiple threads**. You may experiment to find an optimal number of threads to maximize your throughput while minimizing the incurred parallelization overhead.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "Before the deadline (June 2nd, 2025 23:59), you must submit a package file (`zip`) via Moodle containing the following:\n",
    "\n",
    "1. Source code of your implementation;\n",
    "2. Documentation file (`pdf`, max 3 pages);\n",
    "3. Link to the produced index structures (stored on Google Drive).\n",
    "\n",
    "Your `indexer.py` and `processor.py` files must be located at the root of your submitted zip file. You must guarantee that the index generated by your `indexer.py` can be correctly processed by your `processor.py`.\n",
    "\n",
    "## Grading\n",
    "\n",
    "This assignment is worth a total of 15 points distributed as:\n",
    "\n",
    "- 10 points for your _implementation_, assessed based on the quality of your source code, including its overall organization (modularity, readability, indentation, use of comments) and appropriate use of data structures, as well as on how well it abides by the aforementioned indexing and query processing policies.\n",
    "- 5 points for your documentation, assessed based on a short (pdf) report [$^7$][Note_7] describing your implemented data structures and algorithms, their computational complexity, as well as a discussion of their empirical efficiency (e.g. the time elapsed during each step of indexing and query processing, the speedup achieved via parallelization). Your documentation should also include a characterization of your produced inverted index, including (but not limited to) the following statistics: number of documents, number of tokens, number of inverted lists, and a distribution of the number of postings per inverted list. Likewise, you should include a characterization of the results produced for the provided test queries, such as the number of matched documents per query as well as statistics of the score distributions for the two implemented ranking functions (TFIDF and BM25).\n",
    "\n",
    "[Note_7]: https://portalparts.acm.org/hippo/latex_templates/acmart-primary.zip \"Your documentation should be no longer than 3 pages and use the ACM LATEX template (sample-sigconf.tex)\"\n",
    "\n",
    "## Late Submissions\n",
    "\n",
    "Late submissions will be penalized in $2^{(d-1)} - 0.5$ points, where $d > 0$ is the number of days late. In practice, a submission 5 or more days late will result in a zero grade.\n",
    "\n",
    "## Teams\n",
    "\n",
    "This assignment must be performed **individually**. Any sign of plagiarism will be investigated and reported to the appropriate authorities.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87a6bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Resumindo\n",
    "\n",
    "- Fazer Indexer\n",
    "- Fazer Query Processor\n",
    "- Caracterização do index\n",
    "- Usar venv\n",
    "\n",
    "## Indexer\n",
    "\n",
    "- `indexer.py`\n",
    "- 3 argumentos\n",
    "  - `-m <MEMORY>`: a memória disponível para o indexer em megabytes.\n",
    "  - `-c <CORPUS>`: o caminho para o arquivo do corpus a ser indexado.\n",
    "  - `-i <INDEX>`: o caminho para o diretório onde os índices devem ser escritos.\n",
    "- generate JSON\n",
    "  - `Index Size`: o tamanho do índice em megabytes;\n",
    "  - `Elapsed Time`: o tempo decorrido (em segundos) para produzir o índice;\n",
    "  - `Number of Lists`: o número de listas invertidas no índice;\n",
    "  - `Average List Size`: o número médio de postagens por lista invertida.\n",
    "\n",
    "### Corpus\n",
    "\n",
    "- id, title, descriptive text, and keywords\n",
    "- 4,641,784 wikipedia entities\n",
    "- Usar um mini-corpus para testar\n",
    "\n",
    "### Indexing Policies\n",
    "\n",
    "- Para cada documento:\n",
    "  - parse\n",
    "  - tokenize\n",
    "  - index\n",
    "- Respeitar o limite de memória\n",
    "- Produzir 3 arquivos separados\n",
    "  - Índice invertido\n",
    "  - Index de documentos\n",
    "  - Lexicon de termos\n",
    "- **Policies:**\n",
    "  1. Pre-processamento\n",
    "    - Stopword removal\n",
    "    - Stemming\n",
    "    - Outras técnicas\n",
    "  2. Gerenciamento de memória\n",
    "    - Produzir índices parciais em memória\n",
    "    - Fazer merge no disco\n",
    "    - `psutil.Process(os.getpid()).memory_info().rss`\n",
    "    - Assuma `-m 1024`\n",
    "  3. Paralelização\n",
    "    - Indexação em múltiplas threads\n",
    "  4. Compressão (extra)\n",
    "     - Implementar compressão para entradas do índice (gamma, unary, etc.)\n",
    "\n",
    "## Query Processor\n",
    "\n",
    "- Arquivo `processor.py`\n",
    "- 3 argumentos\n",
    "  - `-i <INDEX>`: o caminho para um arquivo de índice.\n",
    "  - `-q <QUERIES>`: o caminho para um arquivo com a lista de consultas a serem processadas.\n",
    "  - `-r <RANKER>`: uma string informando a função de ranqueamento (TFIDF ou BM25).\n",
    "- produzir `JSON` para cada consulta: `{'query': 'IR', 'results': [{'id': '0', 'score': 1.0}, ...]}`\n",
    "- Mostrar os 10 melhores resultados (ordenados do maior pro menor score)\n",
    "- **Policies:**\n",
    "  1. Pre-processamento\n",
    "     - Stopword removal\n",
    "     - Stemming\n",
    "  2. Matching\n",
    "     - DAAT conjuntivo\n",
    "  3. Scoring\n",
    "     - TFIDF\n",
    "     - BM25\n",
    "     - Outras variantes da literatura\n",
    "  4. Parallelization (extra)\n",
    "     - Query processing em múltiplas threads\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- Código Fonte (10 pontos)\n",
    "  - Link pros arquivos gerados pelo indexer (Google Drive)\n",
    "    - Índice invertido\n",
    "    - Index de documentos\n",
    "    - Lexicon de termos\n",
    "  - na raiz:\n",
    "  - `indexer.py`\n",
    "  - `processor.py`\n",
    "- Documentação (pdf, 3 páginas) (5 pontos)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068b754",
   "metadata": {},
   "source": [
    "# Código\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c66c7b",
   "metadata": {},
   "source": [
    "## Indexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78272319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate a partial corpus from a given text file. \"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "def generate_partial_corpus(input_file, output_file, num_lines):\n",
    "    \"\"\"Generate a partial corpus from the input file.\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.writelines(lines[:num_lines])\n",
    "\n",
    "    print(f\"Partial corpus generated with {num_lines} lines in {output_file}.\")\n",
    "\n",
    "\n",
    "generate_partial_corpus('source/input/corpus.jsonl', 'source/input/10k_corpus.jsonl', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd2fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" asd \"\"\"\n",
    "import json\n",
    "\n",
    "def analyse_corpus():\n",
    "    \"\"\"Analyse the corpus and return a dictionary with the results.\"\"\"\n",
    "    path = \"source/input/corpus.jsonl\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        print(len(lines))\n",
    "        for i, line in enumerate(lines):\n",
    "            if i < 5:\n",
    "                msg = line.strip() + \"\\n\"\n",
    "                msg += str(json.loads(line)) + \"\\n\"\n",
    "                print(msg)\n",
    "                # print(line.keys())\n",
    "    \n",
    "analyse_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bcf10",
   "metadata": {},
   "source": [
    "# Gerenciamento de Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62cbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_processing(doc, mem_limit, index_path):\n",
    "    \"\"\" Process a single document \"\"\"\n",
    "    # Limpa variáveis de execuções anteriores\n",
    "    local_vars = ['doc_info', 'parsed_doc', 'tokens', 'index_entries']\n",
    "    for var in local_vars:\n",
    "        if var in locals():\n",
    "            del locals()[var]\n",
    "\n",
    "    # Processa o documento\n",
    "    doc = json.loads(doc)\n",
    "    # ...resto do código..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def doc_processing(doc, mem_limit, index_path):\n",
    "    \"\"\" Process a single document \"\"\"\n",
    "    # Força coleta de lixo\n",
    "    gc.collect()\n",
    "\n",
    "    doc = json.loads(doc)\n",
    "    # ...resto do código..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1428bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def doc_processing(doc, mem_limit, index_path):\n",
    "    \"\"\" Process a single document \"\"\"\n",
    "    # Limpa variáveis explicitamente\n",
    "    local_vars = ['doc_info', 'parsed_doc', 'tokens', 'index_entries']\n",
    "    for var in local_vars:\n",
    "        if var in locals():\n",
    "            del locals()[var]\n",
    "\n",
    "    # Força coleta de lixo\n",
    "    gc.collect()\n",
    "\n",
    "    doc = json.loads(doc)\n",
    "    # ...resto do código..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c192148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "process = psutil.Process()\n",
    "memory_info = process.memory_info().rss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f4482",
   "metadata": {},
   "source": [
    "# Bibliotecas\n",
    "\n",
    "## NLTK\n",
    "\n",
    "- works with human language data\n",
    "- suite of text processing libraries for:\n",
    "  - classification\n",
    "  - [tokenization](https://www.nltk.org/api/nltk.tokenize.html)\n",
    "    - `nltk.word_tokenize(sentence)`\n",
    "  - [stemming](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem)\n",
    "  - tagging\n",
    "  - parsing\n",
    "  - semantic reasoning\n",
    "  - Other\n",
    "    - Vocabulary\n",
    "    - [JSON](https://www.nltk.org/api/nltk.jsontags.html)\n",
    "    - Stopwords\n",
    "      - `import nltk`\n",
    "      - `from nltk.corpus import stopwords`\n",
    "      - `nltk.download('stopwords')`\n",
    "      - `print(stopwords.words('english'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018ec9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2849e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joaov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea0a1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk stopwords: 198\n",
      "my stopwords: 179\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "with open('source/input/stopwords.json', 'r', encoding='utf-8') as f:\n",
    "    my_stopwords = json.load(f)\n",
    "\n",
    "print(\"nltk stopwords:\", len(nltk_stopwords))\n",
    "print(\"my stopwords:\", len(my_stopwords))\n",
    "\n",
    "print(sorted(nltk_stopwords))\n",
    "print(sorted(my_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8399a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\joaov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
