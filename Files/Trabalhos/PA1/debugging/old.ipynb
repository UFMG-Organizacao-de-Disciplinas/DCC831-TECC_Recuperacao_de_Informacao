{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f516089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Variáveis globais \"\"\"\n",
    "\n",
    "# Substituir posteriormente como um parâmetro enviado pelo comando.\n",
    "# SEEDS = './Seeds/seeds-2024711370.txt'\n",
    "# LIMIT = 1000\n",
    "# DEBUG_MODE = True\n",
    "\n",
    "# Variáveis de controle\n",
    "# DELAY = 100\n",
    "# MAX_THREADS = 10\n",
    "# visited_URLs = set()\n",
    "# frontier = {}\n",
    "# trash_can = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helper functions: Timestamp, Node, Minimal URLs, Debug \"\"\"\n",
    "\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\" Retorna o timestamp atual em segundos desde 1970 \"\"\"\n",
    "    return int(datetime.datetime.now().timestamp())\n",
    "\n",
    "\n",
    "def get_node():\n",
    "    \"\"\" Retorna um nó vazio \"\"\"\n",
    "    node = {\n",
    "        'URL': None,\n",
    "        'Title': None,\n",
    "        'Timestamp': get_timestamp(),\n",
    "        'Text': None,\n",
    "\n",
    "        'time_elapsed': None,\n",
    "        'raw_content': None,\n",
    "        'quantity': 0,\n",
    "        'url': None,\n",
    "\n",
    "        'visited': {},\n",
    "        'frontier': {},\n",
    "        'trash_can': {},\n",
    "        'children': {},\n",
    "    }\n",
    "    return node\n",
    "\n",
    "\n",
    "def get_minimal_url(URL):\n",
    "    \"\"\" Retorna a URL mínima\n",
    "    A intenção é encontrar a URL mínima que leva ao mesmo conteúdo.\n",
    "    Linf. Form.: htt(p|ps)://(www.|)\n",
    "    \"\"\"\n",
    "\n",
    "    https = 'https://'\n",
    "    http = 'http://'\n",
    "    www = 'www.'\n",
    "\n",
    "    no_start = ''\n",
    "    only_www = ''\n",
    "    only_https = ''\n",
    "    only_https_www = ''\n",
    "    only_http = ''\n",
    "    only_http_www = ''\n",
    "\n",
    "    minimal_urls = []\n",
    "\n",
    "    return minimal_urls\n",
    "\n",
    "\n",
    "def debug_as_json(node):\n",
    "    \"\"\" Retorna um nó em formato JSON \"\"\"\n",
    "    print(json.dumps(node, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Visiting seeds \"\"\"\n",
    "\n",
    "\n",
    "def visit_seed(seed):\n",
    "    \"\"\" Função para visitar um seed e armazenar o conteúdo. \"\"\"\n",
    "    try:\n",
    "        # Minimize seed URL\n",
    "        # seed = get_minimal_url(seed)\n",
    "        response = requests.get(seed, timeout=5)\n",
    "        mime = response.headers.get('Content-Type', '').split(';')[0]\n",
    "        if response.status_code == 200 and mime == 'text/html':\n",
    "            new_node = get_node()\n",
    "            # Explorando respostas\n",
    "            info = {\n",
    "                'url': response.url,\n",
    "                'status_code': response.status_code,\n",
    "                # 'headers': dict(response.headers),\n",
    "                'encoding': response.encoding,\n",
    "                'text': response.text,\n",
    "                # 'content': response.content.decode('utf-8', errors='ignore'),\n",
    "                'time_elapsed': response.elapsed.microseconds,\n",
    "            }\n",
    "            if DEBUG_MODE:\n",
    "                # debug_as_json(info)\n",
    "                # print(info['text'])\n",
    "                # print(20*'\\n')\n",
    "                # print(info['content'])\n",
    "                pass\n",
    "\n",
    "            new_node['url'] = seed\n",
    "            new_node['quantity'] += 1\n",
    "            new_node['raw_content'] = response.text\n",
    "            new_node['time_elapsed'] = response.elapsed.microseconds\n",
    "\n",
    "            return new_node\n",
    "        else:\n",
    "            trash_can[seed] = response\n",
    "            if DEBUG_MODE:\n",
    "                print(f'Failed to visit {seed}: {response.status_code}')\n",
    "            return trash_can\n",
    "    except requests.RequestException as e:\n",
    "        if DEBUG_MODE:\n",
    "            print(f'Error visiting {seed}: {e}')\n",
    "\n",
    "\n",
    "# for seed in base_seeds:\n",
    "#     if DEBUG_MODE:\n",
    "#         print(f'Seed: {seed}')\n",
    "#     visited_URLs.add(seed)\n",
    "#     new_node = visit_seed(seed)\n",
    "#     debug_as_json(new_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Processing sitemaps and robots.txt \"\"\"\n",
    "\n",
    "\n",
    "def get_sitemap(seed):\n",
    "    \"\"\" Função para obter o sitemap de um site. \"\"\"\n",
    "    sitemap_url = seed + '/sitemap.xml'\n",
    "    try:\n",
    "        response = requests.get(sitemap_url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            trash_can[sitemap_url] = response\n",
    "            if DEBUG_MODE:\n",
    "                print(\n",
    "                    f'Failed to get sitemap for {seed}: {response.status_code}')\n",
    "            return trash_can\n",
    "    except requests.RequestException as e:\n",
    "        if DEBUG_MODE:\n",
    "            print(f'Error getting sitemap for {seed}: {e}')\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_robots_txt(seed):\n",
    "    \"\"\" Função para obter o arquivo robots.txt de um site. \"\"\"\n",
    "    robots_url = seed + '/robots.txt'\n",
    "    try:\n",
    "        response = requests.get(robots_url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            trash_can[robots_url] = response\n",
    "            if DEBUG_MODE:\n",
    "                print(\n",
    "                    f'Failed to get robots.txt for {seed}: {response.status_code}')\n",
    "            return trash_can\n",
    "    except requests.RequestException as e:\n",
    "        if DEBUG_MODE:\n",
    "            print(f'Error getting robots.txt for {seed}: {e}')\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_robots_rules(robots_txt):\n",
    "    \"\"\" Função para processar o arquivo robots.txt e retornar as regras. \"\"\"\n",
    "    rules = {'sitemaps': set(), 'user_agents': {}}\n",
    "    lines = robots_txt.split('\\n')\n",
    "    user_agent = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('User-agent:'):\n",
    "            user_agent = line.split(':')[1].strip()\n",
    "            rules['user_agents'][user_agent] = {'allow': [], 'disallow': []}\n",
    "        elif line.startswith('Disallow:') and user_agent:\n",
    "            path = line.split(':')[1].strip()\n",
    "            rules['user_agents'][user_agent]['allow'].append(path)\n",
    "        elif line.startswith('Allow:') and user_agent:\n",
    "            path = line.split(':')[1].strip()\n",
    "            rules['user_agents'][user_agent]['disallow'].append(path)\n",
    "        elif line.startswith('Sitemap:'):\n",
    "            sitemap_url = line.split(': ')[1].strip()\n",
    "            rules['sitemaps'].add(sitemap_url)\n",
    "    return rules\n",
    "\n",
    "\n",
    "def process_sitemaps(robots_rules):\n",
    "    \"\"\" empty all xml sitemap files \"\"\"\n",
    "    def process_sitemap(sitemap):\n",
    "        \"\"\" Função para processar cada sitemap. \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(sitemap, timeout=5)\n",
    "            mime = response.headers.get('Content-Type', '').split(';')[0]\n",
    "            print(f'Processing sitemap: {sitemap}; mime: {mime}')\n",
    "            # MIMEs: text/xml, application/xml\n",
    "            if response.status_code == 200:\n",
    "                soup = bs.BeautifulSoup(response.content, 'lxml')\n",
    "                # try:\n",
    "                #     soup = bs.BeautifulSoup(response.content, 'xml')\n",
    "                # except (bs.FeatureNotFound):\n",
    "                #     soup = bs.BeautifulSoup(response.content, 'html.parser')\n",
    "                print(soup)\n",
    "            #     urls = [url.text for url in soup.find_all('loc')]\n",
    "            #     for url in urls:\n",
    "            #         if url not in visited_URLs:\n",
    "            #             visited_URLs.add(url)\n",
    "            #             new_node = visit_seed(url)\n",
    "            #             debug_as_json(new_node)\n",
    "            # else:\n",
    "            #     trash_can[sitemap] = response\n",
    "            #     if DEBUG_MODE:\n",
    "            #         print(f'Failed to process sitemap {sitemap}: {response.status_code}')\n",
    "        except requests.RequestException as e:\n",
    "            if DEBUG_MODE:\n",
    "                print(f'Error processing sitemap {sitemap}: {e}')\n",
    "\n",
    "    for sitemap in robots_rules['sitemaps']:\n",
    "        process_sitemap(sitemap)\n",
    "\n",
    "\n",
    "# print(base_seeds)\n",
    "\n",
    "def process_seed(seed):\n",
    "    \"\"\" Função para processar cada seed em uma thread. \"\"\"\n",
    "    if DEBUG_MODE:\n",
    "        print(f'Processing seed: {seed}')\n",
    "    robots_txt = get_robots_txt(seed)\n",
    "    robots_rules = get_robots_rules(robots_txt)\n",
    "    processed_sitemaps = process_sitemaps(robots_rules)\n",
    "\n",
    "\n",
    "# for seed in base_seeds:\n",
    "#     process_seed(seed)\n",
    "#     print(100*'=')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
