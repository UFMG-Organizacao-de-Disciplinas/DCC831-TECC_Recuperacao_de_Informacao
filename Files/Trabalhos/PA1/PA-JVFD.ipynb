{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enunciado do Trabalho\n",
    "\n",
    "Universidade Federal de Minas Gerais\n",
    "\n",
    "Departamento de Ciência da Computação\n",
    "\n",
    "TCC/TSI/TECC: Information Retrieval\n",
    "\n",
    "## Programming Assignment #1 - Web Crawler\n",
    "\n",
    "- **Deadline:** Apr 28th, 2025 23:59 via Moodle\n",
    "\n",
    "### Overview\n",
    "\n",
    "The goal of this assignment is to implement a crawler capable of fetching a mid-sized corpus of webpages in a short time frame while respecting the politeness constraints defined by each crawled website. In addition to the source code of your implementation and the actual crawled documents, your submission must include a characterization of these documents.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "You must use Python 3 for this assignment. Your code must run in a virtual environment **using only the libraries included** in the provided `requirements.txt` file. Execution errors due to missing libraries or incompatible library versions will result in a zero grade. To make sure you have the correct setup, you can test it in one of the [Linux machines provided by the Department of Computer Science $^{1}$][Link_CRC] using the following commands:\n",
    "\n",
    "[Link_CRC]: <https://www.crc.dcc.ufmg.br/doku.php/infraestrutura/laboratorios/linux>\n",
    "\n",
    "```bash\n",
    "$ python3 -m venv pa1\n",
    "$ source pa1/bin/activate\n",
    "$ pip3 install -r /path/to/requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "### Execution\n",
    "\n",
    "Your implementation should include a `main.py` file, which will be executed in the same virtual environment described above, as follows:\n",
    "\n",
    "```bash\n",
    "$ python3 main.py -s <SEEDS> -n <LIMIT> [-d]\n",
    "```\n",
    "\n",
    "with the following arguments:\n",
    "\n",
    "- **-s <SEEDS>**: the path to a file containing a list of seed URLs (one URL per line) for initializing the crawling process.\n",
    "- **-n <LIMIT>**: the target number of webpages to be crawled; the crawler should stop its execution once this target is reached.\n",
    "- **-d**: (optional argument) run in debug mode (see below).\n",
    "\n",
    "### Debugging\n",
    "\n",
    "When executed in debugging mode (i.e. when -d is passed as a command-line argument), your implementation must print a record of each crawled webpage to [standard output $^2$][StandardOutput] as it progresses. Such a record must be formatted as a JSON document containing the following fields:\n",
    "\n",
    "[StandardOutput]: <https://en.wikipedia.org/wiki/Standard_streams#Standard_output_(stdout)>\n",
    "\n",
    "- `URL`, containing the page URL;\n",
    "- `Title`, containing the page title;\n",
    "- `Text`, containing the first 20 words from the page visible text;\n",
    "- `Timestamp`, containing the [Unix time $^3$][UnixTime] when the page was crawled.\n",
    "\n",
    "[UnixTime]: <https://en.wikipedia.org/wiki/Unix_time>\n",
    "\n",
    "The following example illustrates the required debugging output format for the first webpage fetched during a crawling execution:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"URL\": \"https://g1.globo.com/\",\n",
    "    \"Title\": \"G1 - O portal de notícias da Globo\",\n",
    "    \"Text\": \"Deseja receber as notícias mais importantes em tempo real? Ative as notificações do G1! Agora não Ativar Gasto familiar Despesa\",\n",
    "    \"Timestamp\": 1649945049\n",
    "}\n",
    "```\n",
    "\n",
    "### Crawling Policies\n",
    "\n",
    "Your implementation must keep a frontier of URLs to be crawled, which must be initialized with the seed URLs provided as input to you with the -s argument. For each URL consumed from the frontier, your implementation must fetch the corresponding webpage, parse it, store the extracted HTML content in the local corpus, and enqueue the extracted outlinks in the frontier to be crawled later. In addition to this standard workflow, **your implementation must abide by the following crawling policies:**\n",
    "\n",
    "1. *Selection Policy:* Starting from the provided seed URLs, your implementation **must only follow discovered links to HTML pages** (i.e. resources with MIME type `text/html`). To improve coverage, you may optionally choose to limit the crawling depth of any given website.\n",
    "2. *Revisitation Policy:* Because this is a one-off crawling exercise, you **must not revisit a previously crawled webpage**. To ensure only new links are crawled, you may choose to normalize URLs and check for duplicates before adding new URLs to the frontier.\n",
    "3. *Parallelization Policy:* To ensure maximum efficiency, you **must parallelize the crawling process across multiple threads**. You may experiment to find an optimal number of threads to maximize your download rate while minimizing the incurred parallelization overhead.\n",
    "4. *Politeness Policy:* To avoid overloading the crawled websites, your implementation [**must abide by the robots exclusion protocol** $^4$][4]. Unless explicitly stated otherwise in a `robots.txt` file, you must obey a delay of at least 100ms between consecutive requests to the same website.\n",
    "5. *Storage Policy:* As the main target for this assignment, your implementation **must crawl and store a total of 100,000 unique webpages**. The raw HTML content of the crawled webpages must be packaged using the [WARC format $^5$][5], with 1,000 webpages stored per WARC file (totalling 100 such files), compressed with gzip to reduce storage costs.\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "Before the deadline (Apr 28th, 2025 23:59), you must submit a package file (zip) via Moodle containing the following:\n",
    "\n",
    "1. Source code of your implementation;\n",
    "2. Link to your crawled corpus (stored on Google Drive);\n",
    "3. Documentation file (pdf, max 2 pages).\n",
    "\n",
    "### Grading\n",
    "\n",
    "This assignment is worth a total of 15 points distributed as:\n",
    "\n",
    "- 10 points for your *implementation*, assessed based on the quality of your source code, including its overall organization (modularity, readability, indentation, use of comments) and appropriate use of data structures, as well as on how well it abides by the five aforementioned crawling policies.\n",
    "- 5 points for your *documentation*, assessed based on a [short (pdf) report $^6$][6] describing your implemented data structures and algorithms, their computational complexity, as well as a discussion of their empirical efficiency (e.g. the download rate throughout the crawling execution, the speedup achieved as new threads are added). Your documentation should also include a characterization of your crawled corpus, including (but not limited to) the following statistics: total number of unique domains, size distribution (in terms of number of webpages) per domain, and size distribution (in terms of number of tokens) per webpage.\n",
    "\n",
    "### Teams\n",
    "\n",
    "This assignment must be performed individually. Any sign of plagiarism will be investigated and reported to the appropriate authorities.\n",
    "\n",
    "[4]: <https://en.wikipedia.org/wiki/Robots_exclusion_standard>\n",
    "[5]: <https://en.wikipedia.org/wiki/Web_ARChive>\n",
    "[6]: <https://portalparts.acm.org/hippo/latex_templates/acmart-primary.zip> \"Your documentation should be no longer than 2 pages and use the ACM LATEX template (sample-sigconf.tex)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## To-Do's\n",
    "\n",
    "### Execução\n",
    "\n",
    "- [ ] Install `requirements.txt` libraries\n",
    "- Definir os parâmetros de inicialização\n",
    "  - [ ] Path to Seeds\n",
    "  - [ ] Target Number of Webpages\n",
    "  - [ ] Debug Mode\n",
    "\n",
    "### Debugging Prints\n",
    "\n",
    "- [ ] URL\n",
    "- [ ] Title\n",
    "- [ ] Text\n",
    "- [ ] Timestamp (Unix time)\n",
    "\n",
    "### Follow Policies\n",
    "\n",
    "- Selection Policy\n",
    "  - [ ] only MIME type `text/html`\n",
    "  - [ ] Limit crawling depth of any given website (opcional)\n",
    "- Revisitation Policy\n",
    "  - [ ] Normalize URLs before adding to frontier\n",
    "- Parallelization Policy\n",
    "  - [ ] Parallelize the crawling process across multiple threads\n",
    "- Politeness Policy\n",
    "  - [ ] Obey the `robots.txt` file\n",
    "  - [ ] Delay of at least 100ms between consecutive requests to the same website\n",
    "- Storage Policy\n",
    "  - [ ] Store 100,000 unique webpages\n",
    "  - [ ] Package using WARC format\n",
    "  - [ ] Compress with gzip to reduce storage costs\n",
    "  - [ ] Store at Google Drive\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- 2 pages (pdf)\n",
    "  - [ ] ACM LATEX template (sample-sigconf.tex)\n",
    "    - Data Structures\n",
    "    - Algorithms\n",
    "    - Computational Complexity\n",
    "    - Empirical Efficiency\n",
    "    - Crawled Corpus Characterization\n",
    "      - Total number of unique domains\n",
    "      - Size distribution (in terms of number of webpages) per domain\n",
    "      - Size distribution (in terms of number of tokens) per webpage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ideias:\n",
    "\n",
    "- Explorar urls\n",
    "  - Vasculhar o sitemap\n",
    "  - Árvore de aprendizado descritivo pra armazenar links\n",
    "    - Armazenar logo quando achar?\n",
    "    - Percorrer a árvore pra poder extrair os dados?\n",
    "    - Percorrer enquanto se preenche?\n",
    "\n",
    "- Paralelismo\n",
    "  - Adiar o processamento de threads que passarem de algum determinado tempo de processamento.  \n",
    "  - Uma nova thread para cada nó de árvore? Não... Threads demais.\n",
    "    - Qual limite?\n",
    "\n",
    "- MIME = CONTENT-TYPE\n",
    "- MIME = MIMETYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "minha_seed = 'seeds-2024711370.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
